\chapter{Related Work}

Robot localization and mapping has been treated as fundamental problem in robotics filed. In general, they are addressed as Simultaneous Localization And Mapping (SLAM)  \cite{guivant2012internet, robledo2011outdoor, lohrsemantic, 58segal2009generalized,42montemerlo2007fastslam, 08weingarten2005ekf}. I.e., while the robot is navigating and planning its movements to avoid obstacles, it maps the environment and localizes itself using the map. The main disadvantage of online SLAM algorithms is the inability to revisit sensors data, in special to handle the loop closure problem \cite{10thrun2006graph}.

Therefore, a Graph-SLAM has been considering since it has a natural way to handle with loop closure problem \cite{10thrun2006graph, Levinson-RSS-07, LevinsonT10}, [15]. The Graph-SLAM algorithm proposed in \cite{10thrun2006graph} is a Full-SLAM algorithm that uses the whole sensor data to estimate the robot path and the map. In the Graph-SLAM, the nodes represent model variables (poses and map variables) and the edges represent sensors measurements and its respective covariance. However, the number of landmark in large outdoors areas tends to be huge being computationally unfeasible deal with this amount of data during the path optimization. Therefore, mapping large scale urban scenarios is addressed marginalizing the map variables to become restriction between poses \cite{Levinson-RSS-07, LevinsonT10}. 

Graph-SLAM algorithms are inherent offline, hence, to localize a robot during its navigation a Monte Carlo Localization or Kalman Filter based Localization must be considered \cite{Thrun00j}, [16]. Generally, to build the map in outdoor huge areas a GPS, IMU, robot odometry, and LiDAR are used. The main advantage of the use GPS is because it has limited error that never grows while the robot is moving. Nevertheless, the GPS signal suffers from outages or occlusions problems, which are intensified in places such as urban canyons, indoor places, underground mines, etc.

To overcome the issues in GPS denied places, in [15] they used available aerial photography as a map to localize the robot just once in the course, and generate the graph nodes by the localization returned poses. Thereafter, a Graph-SLAM is applied to produce the most likely path that will be used in the mapping. The alternative to perform the localization by using aerial map images is also used as the main localization of the robot in outdoor environment [17]â€“[20]. However, it fails when robotic vehicles are crossing tunnel or in a place with roof of trees. Moreover, in a featureless environment like mines, it would not work.

Alternatively, the road-map can be used to perform the robot localization in these GPS denied sites. Therefore, the road-map has been used to address the global localization problem \cite{guivant2007global,guivant2010robust}, [21]. In [21] they used the OpenStreetMap to globally localize a vehicle in unban roads. They do so, comparing the short historical of the visual odometry [22] against to the road-map using Monte Carlo Localization. In the proposals presented in \cite{guivant2007global, guivant2010robust}, rather than visual odometry, they use 200 meters of the vehicle dead-reckoning to perform the same pose correction. In those solutions, the robot position belief is high during the robot turn, but in a straight line the covariance can increase causing miss localization. 

One common feature in road-maps is the intersection between roads. Thereby, in this work, we are exploring this characteristic to reduce the position uncertainty in the car position. To perform the intersection detection a 3D LiDAR has been used to create a local map of environment. The local map is used to measure the road width. If the width is bigger than a threshold the car is on a road intersection.

Since the dead-reckoning is used to compare against the road-map to perform the particle correction in Monte Carlo localization, as well as to build the local map. Wheel slips 
during the car acceleration, turn and breaking may affect negatively the pose correction and also the local map construction. Thus, in this work, it has been corrected by scan-matching algorithm like Generalized Iterative Closest Point (well known as Generalized-ICP) in the case of this work [23].


After the Darpa Urban Challenge \cite{DBLP:conf/darpa/2009},  the automotive industry has increased the efforts to develop an autonomous vehicle. Most of the solutions depend on a good GPS receiver to localize the vehicles globally and absolutely, mainly for outdoor environments~\cite{bacha2008odin,leonard2008perception,Montemerlo:2008:JSE:1405647.1405651,Urmson:2008:ADU:1395073.1395077,Auat2013}. However, standard GPS signal is not always reliable when navigating on the roads or for indoor environments. To solve this problem, the approach proposed by the \emph{Stanford autonomous vehicle} used a map, built beforehand, to localize the car in the environment~\cite{Levinson-RSS-07,LevinsonT10}. The map is constructed using a Graph-SLAM algorithm fusing the data from the GPS, the IMU, the wheels odometry and the LiDAR. This way, when the GPS signal failed, a continuous localization could still be done using the other sensors.  However, it is not clear if the Graph-SLAM algorithm could be applied to GPS-denied 
environments, like a stockyard, a parking lot, and industrial plants. Other issue is how to perform the initial localization, i.e. the vehicle initialization, without a GPS signal. In addition, agricultural processes and the mining industry would also benefit from a GPS-independent but reliable localization system, as mentioned in~\cite{Auat2013}.
There is a high expectation that, in the future, autonomous vehicles will be used to provide accessibility to injured people, reduce time and cost of transportation systems, and offer comfort to people that do not want (or cannot) drive. Beyond that, autonomous vehicles have potential to reduce traffic accidents once robots may have an improved capacity to observe the world, and they are not subject to excessive driver workloads such as tiredness, anger, drunkenness, hurry or stress. \cite{01DARPAUrban2007}, claim that future vehicles with automated sensing and control technologies have potential to save millions of people lives.

Several  agencies and companies are in progress around the world motivated by the development of autonomous vehicles. The United States of America's Defense Advanced Research Projects Agency (DARPA), for example, organized three competitions where research teams were challenged to build solutions for autonomous  navigation on general roads \cite{01DARPAUrban2007,02DARPAGrand2005}. Google driverless car has shown increasing quality with 700,000 accident-free miles traveled in USA roads \cite{03urmson2012self}. The AutoNOMOS labs of the Free University of Berlin developed the MadeInGermany vehicle which received a certification to drive autonomously in the Berlin state \cite{04AutoNOMOS}. Moreover, car factories like Volvo and Mercedes-Benz have been investing in projects to develop self-driving vehicles and they already presented autonomous driving experiments on conventional roads \cite{05Volvo,06Mercedes-Benz}.

To achieve autonomous navigation, robotic vehicles must be able to sense the world, create a map, and use it to localize itself and plan actions. Several approaches were proposed to achieve these goals \cite{07durrant2006simultaneous,08weingarten2005ekf,09williams2001efficient,10thrun2006graph,11dissanayake2002map,12duckett2000learning}, and, until now, the most successful one was based on an offline map construction, and the posterior online localization and navigation using the created map (proposed and tested in \cite{13levinson2007map}). The main advantage of this approach is the freedom to use time consuming techniques to enhance map quality (e.g. loop closure correction, etc.) without the risk of losing on-the-fly localization due to the lack of resources \cite{10thrun2006graph}. In general, Simultaneous Localization and Mapping (SLAM) techniques are used to build the map, and Monte Carlo Localization  \cite{14thrun2001robust} or Kalman Filter-based methods \cite{15roumeliotis2000bayesian} are used to perform robotic localization. Examples of algorithms used to perform robotic navigation are the A* algorithm \cite{16likhachev2005anytime} and the Rapidly exploring Random Tree (RRT) algorithm \cite{17lavalle2000rapidly}, among others.

Most probabilistic SLAM algorithms hypothesize sensors subject to zero-mean Gaussian noise. And even being a topic of paramount importance, just a few articles approach the problem of estimation and integration of non-Gaussian components in sensor data. In \cite{31kummerle2013state} and \cite{20perera2003sensor}, the authors propose the estimation of sensors' biases using a probabilistic framework. They augmented the state of the probabilistic estimator by composing each one of the robot poses with associated bias variables. By doing so, both poses and biases are calculated together during the estimation process. One important feature of this approach is the ability to change (even dramatically) the biases values on-the-fly in every instant. Although this type of implementation guarantee recoverability to unpredicted events, the freedom of the biases may lead to incorrect parameters updates to compensate imprecisions in the measurement model (e.g., mismatching of landmarks) and the motion model (navigating in slippery terrains, drifting). In this work, we explore a novel approach by assuming constant biases and calculating its values in a preprocessing off-line phase. This assumption simplifies the problem and avoids rough changes in the bias estimation. However, it is important to clarify that our approach is unable to deal with on-the-fly intercurrences (e.g., changes in tire pressure, wheel balancing and/or wheel alignment), which is the future research of the authors.

Kalman Filter-based SLAM algorithms (e.g., Extended Kalman Filter (EKF-SLAM) \cite{08weingarten2005ekf}, Unscented Kalman Filter SLAM (UKF-SLAM) \cite{26thrun2005probabilistic} and Iterated Sigma Point Kalman Filter SLAM (ISPKF-SLAM) \cite{33sibley2006iterated}) update the estimation of the pose, the map and their uncertainties using a sequence of prediction and correction steps. During the prediction step, the motion model is used to expand the parameters search space by raising their covariance. In the correction step, on the other hand, the measurement model is used to enhance the probability of likely regions of the search space and shrink the covariance around these values \cite{07durrant2006simultaneous,34dissanayake2001solution,35maybeck1982stochastic}. Successful applications using Kalman Filter-based SLAM algorithms can be found in \cite{08weingarten2005ekf,36cheein2010slam,37auat2011optimized,38mallios2010ekf,39guivant2001optimization,40guivant2002simultaneous}.

Even being the standard solution to the SLAM problem for several years, the EKF-SLAM algorithm has drawbacks that prevent its use in outdoor and/or large-scale environments. Once the poses and the map are represented in a joint state, adding new poses and new landmarks increase the dimensions of the state's mean and covariance matrices. Additionally, visiting poses and observing landmarks lead to a computational expensive update of the full mean and covariance matrices. This unbound growth of the state matrices makes the solution unfeasible to large-scale applications. Besides this drawback, the EKF-SLAM can diverge due to several other conditions, such as landmark matching errors bigger than the expected covariance and the presence of strong nonlinearities or discontinuities in the motion model and in the measurement model.

In \cite{41montemerlo2002fastslam} and \cite{42montemerlo2007fastslam}, the FastSLAM and the FastSLAM 2.0 algorithms are presented that are both breakthrough discoveries in the SLAM literature. The FastSLAM algorithm uses Monte Carlo Sampling techniques (also known as Particle Filters) to factorize the SLAM posterior into a product of conditional landmark distributions and a distribution over robot paths. This factorization (called Rao-Blackwellization) enables the introduction of a nonlinear motion model and notably speeds up the SLAM algorithm. In the FastSLAM framework, each particle represents a possible path travelled by the robot and a map.

Thrun and Montemerlo states in \cite{10thrun2006graph} that the main disadvantage of online SLAM algorithms is the inability to revisit sensors data, in special to handle the loop closure problem. Offline techniques introduced in \cite{43lu1997globally} and some subsequent articles \cite{12duckett2000learning,44golfarelli1998elastic,45frese2001simultaneous,46konolige2004large} showed that it is possible to increase precision by memorizing the data until the estimation process is complete and start building the map only when the whole pose set is calculated. In \cite{44golfarelli1998elastic}, the authors showed that the SLAM posterior probability can be modeled as a sparse graph and the optimization of this sparse graph lead to an objective function composed by a sum of nonlinear square restrictions. When optimizing this objective function, it is possible to obtain the most likely map and poses given the sensor data. This new strategy led to the development of GraphSLAM, the state-of-art SLAM algorithm.

The GraphSLAM algorithm proposed in \cite{10thrun2006graph} is a Full-SLAM algorithm that uses the whole sensor data to estimate the robot path and the map. In the GraphSLAM, the nodes represent model variables (poses and map variables) and the edges represent sensors measurements and its respective covariance. In outdoor applications, the number of landmarks tends to be very large and, thus, in \cite{10thrun2006graph},  it is introduced an approximation of the posterior by marginalizing the map variables and transforming them into relationships between poses. By doing so, the map building process is postponed to the moment when the whole pose set is known.

Since GraphSLAM has natural ways to handle the loop closure problem, it was used in this work, instead of the classical online SLAM algorithms (EKF-SLAM or Monte Carlo Particle Filters). In \cite{13levinson2007map}, the authors propose to solve the loop closure problem using map-matching to add loop closure edges in a GraphSLAM framework. Although it is a valid procedure when there are almost zero-error orientation measurements, it requires a limiting amount of computation when the orientation needs to be calculated. To solve this issue, instead of using map-matching, we use the Generalized Iterative Closest Point (GICP) \cite{58segal2009generalized} algorithm as a source of loop closure edges to the GraphSLAM. Experiments and previous work showed that the use of the GICP algorithm provides precise and fast position and orientation estimations \cite{48levinson2010robust}, but it was not used in the field of SLAM.

Once the poses are known, the mapping algorithm reduces to projecting the detected obstacles to the map using the GraphSLAM poses. To detect if one of the laser rays hits an obstacle, in \cite{47leonard2008perception} the authors proposed to compare the angles between the two vectors formed by the connection of three consecutive vertical points of the laser. If the three rays hit a plane surface, both angles will be nearly the same. However, if they hit an obstacle, a significant angular difference will be measured. Although this works well in the border of the obstacles, if all three points hit the obstacle (a wall, for example), both vectors will be vertical, the angular difference will be zero, and no obstacle will be detected. Taking this into consideration, in \cite{22montemerlo2008junior}, the authors propose a new methodology for obstacle detection. It consists of comparing the size of two consecutive laser rays projected in the ground instead of comparing the angular difference between the vectors connecting three LiDAR rays. While the rays hit an obstacle-free plane surface, the size of the projections will have a constant expected value. However, when they hit an obstacle, the size of the projections will be smaller than the expected value. This difference between the expected and the measured values is used to calculate the obstacle probability. Here, we present a new description of this mapping methodology with detailed mathematical formulation.

Finally, it is important to note that previous works focused on mapping structured or unstructured environments while in this work we perform experiments in environments with changing features. 
